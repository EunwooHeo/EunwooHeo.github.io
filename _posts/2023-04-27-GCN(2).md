---
layout: single
title:  "[PAPER 리뷰] (Part 2) SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS"
---

**저자:   Thomas N. Kipf ,  Max Welling** 

지난 글에 이어서, GCN 논문에 대한 리뷰를 진행하겠다.


## **다시 결론 간단 정리**
- 기호 정리
    - $G$ : the undirected graph
    - $A$ : the adjacency matrix
    - $\tilde{A} = A + I$ , where $I$ is the identity matrix.
    - $D$ is the degree diagonal matrix , that is $D\_{ii} = \sum_j A\_{ij}$ for all $i$
    - $\tilde{D} = D+I$   or $\tilde{D}\_{ii} = \sum_j \tilde{A}\_{ij}$ for all $i$.
    - $σ(·)$ : an activation function.
    - for vertex $v \in V(G)$, let $x_v$ be a feature vector in $\mathbb{R}^C$.
    - $X = (x\_{v_1}, x\_{v_2}, \ldots , x\_{v_m})^t$ , that is each $x_v$ is a row vector of $X$.
- $f(X,A)$ : The specific graph-based neural network model 이라고하자.

- 결론적으로, GCN 모델의 layer-wise propagation rule은 다음과 같다.
    
    > $H^{(l+1)} = σ(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$ 
    with $H^{(0)} = X$.

목표는 다음과 같았다.

>1. 위에 제시된 모델에서 각 단계의 $H^{(l)}$ 의 왼쪽에는 $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$를 오른쪽에 $W^{(l)}$를 곱하는 것의 수학적인 직관 제공을 한다. 따라서 앞으로 모든 GNN 관련 논문을 읽기에 용이하도록 한다.
>
>2. 그런데 왜 이게 **Convolutional Network**라고 불리우는 것인지 이해하고, 논문의 공식 도출 과정을 전반적으로 이해한다.

<br>
<br>

이번 글에서는 두 번째 목표로, 왜 이것이 **Convolutional Network**라고 불리우는 것인지 이해해보자. 이것은 본 논문에 자세히 나와있으므로 논문 내용을 정리하며 따라가보자.


## **논문 내용  정리**


## section 1. Introduction

**Contribution**
- 기존의 선행 연구들은 여러가지 한계와 제약이 많았다고 함, 그런데 이를 해결 했다고 함.
- 간단하고 성능이 좋은, 그래프 위에서 ***직접*** 작동하는 모델을 제시 했다고 함.
- 이 방법론이 그래프의 node classification에 얼마나 잘 작동하는지 입증했다고 함.

(자세한것은 논문 참조 )

## section 2. Fast Approximate Convolutions on Graphs (공식 도출 과정 설명)


이미지 데이터의 CNN 처럼, 이 방법론을 GRAPH DATA 위에서 적용할 순 없을까? 에 대한 자연스런 고민으로 다양한 연구가 있어왔다. 대표적으로 다음과 같은 두 가지 접근이 있었는데,
    
>ㄱ. Spatial CNN : 그래프의 인접 점들의 정보로 업데이트 함 (Massage passing)
>
>ㄴ. Spectral CNN : signal 정보를 서로 다른 주기의 spectrum으로 분해하는 것과 같이 정보를 분해 해보자는 접근 
>
>(**Spectral CNN 바로 이번 논문 주제의 접근법**)

  
- 논문 내용
    - **첫번째 STEP**
        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP1.png?raw=true" width="800" height="200"/>
        
        
        💡 저자는 파라미터 $\theta=(\theta_1, \ldots , \theta_N)$에 대하여, $g_{\theta}= diag(\theta_1, \ldots, \theta_N)$ 으로 정의할 때, 
        어떤 vertex 위에 정의된 signal (또는 feature) 에 대하여, convolution 연산을 갑자기 $g_{\theta} \star x = U g_{\theta}U^t x$ 로 정의하였다. 
        어떻게 나온 공식일까? 이것은 바로 convolution 연산과 fourier transformation의 관계로 부터 나온다. Graph Fourier Transformation에 대해 익숙치 않으신 분들은 밑에 (펼치기/접기) 클릭!
        
        <details>
        <summary> Graph Fourier Transformation (펼치기/접기) </summary>

        <div markdown="1">
         
        
        **Graph Fourier Transformation**
        
        Graph Fourier Transformation (GFT) is a mathematical technique that generalizes the traditional Fourier Transformation to graph-structured data. It's used for signal processing, data analysis, and machine learning on graphs.
        
        **Graph Representation**
        
        A graph is represented as $G=(V, E)$, where $V$ is the set of nodes (vertices) and $E$  is the set of edges. Graphs can be further described using an adjacency matrix $A$ and a degree matrix $D$. The Laplacian matrix $L$ of a graph is then calculated as:
        
        > $L = D - A$
        > 
        
        **Graph Signal**
        
        A graph signal is a function that maps the graph nodes to real or complex values. 
        It can be represented as a vector $x$ in  $ℝ^{|V|}$, where  $|V|$  is the number of nodes in the graph.
        
        **Graph Fourier Transformation**
        
        GFT is defined based on the eigendecomposition of the graph Laplacian matrix $L$. 
        The eigenvalues of $L$ are called the graph frequencies, and the corresponding eigenvectors are the graph Fourier modes.
        
        Let $λ₁, λ₂, ..., λ_\{|V|}$ be the eigenvalues of $L$ and $u₁, u₂, ..., u_\{|V|}$ be the corresponding eigenvectors. 
        The Graph Fourier Transform of a graph signal $x$ is given by:
        
        > $\mathcal{F}(x) = [⟨x, u₁⟩, ⟨x, u₂⟩, ..., ⟨x, u_\{|V|}⟩]$
        
        
        where $⟨\cdot \;, \cdot⟩$ denotes the inner product of two vectors.
        That is, 
        
        > $\mathcal{F}(x) = U^tx$ , where $U$ has $u_i$ as $i$th column.
        
        
        **Inverse Graph Fourier Transformation**
        
        The Inverse Graph Fourier Transform is :
        
        > $\mathcal{F}^{-1}(x) = Ux$
        >
        </div>
        </details>
        
        <br>
        
        Graph fourier transformation을 간단히 요약하자면, 그래프 $G$에 대해 Laplacian matrix $L=D-A$를 구한뒤, $L$이 대칭행렬임을 이용하여 직교대각화를 한다. 즉, 직교행렬 $U$를 구하여, $U^t L U = \text{diag}(\lambda_1, \ldots , \lambda_k)$로 나타낸다. 직교행렬을 $U = (u_1^t \; u_2^t \;  \cdots \; u_k^t)$ column vector으로 표현했을 때, 직교행렬 $U^t$를 왼쪽에 곱한 결과인 $U^t x$은 벡터 $x$를 벡터 $u_i$와 내적한 것들의 성분과 같다. 즉 $U^t x = (<u_1,x>,<u_2,x>, \ldots, <u_k,x>)^t$. 따라서, 직교 좌표계 $\{ u_1^t, \; u_2^t, \;  \ldots, \; u_k^t \}$에 대해 분해하는 것이므로 이러한 과정을 graph fourier transform이라 한다. (참고: $U$에 종속되는 개념)

        그럼 이제, graph fourier transformation이 $\mathcal{F}(x)=U^tx$ 인 것을 기억하고 다음으로 넘어가보자. Fourier transformation Theory에서는 유명한 공식이 있다. 그건 바로 다음과 같은Fourier transformation과 Convolution 연산의 관계이다. 
        > 두 함수에 관한 convolution을 $f*g$로 표현하자. 또 함수 $f$에 관한 Fourier transformation을 $\mathcal{F}(f)$라 하자. 그러면 **(convolution 연산을 한 뒤 Fourier transformation 한 것)** 과 **(Fourier transformation 한 뒤 convolution 연산을 한 것이 같다)**!
        >
        > 이를 식으로 표현하면 $\mathcal{F}(f*g)=\mathcal{F}(f)\cdot \mathcal{F}(g)$ 이로부터, $f*g=\mathcal{F}^{-1}(\mathcal{F}(f)\cdot \mathcal{F}(g))$

           
        그럼 이제 위의 모티브에 따라,
        임의의 두 벡터 $g , x \in \mathbb{R}^N$에 대해, 그래프 $G$에 대한 normalized laplacian $L=D^{-1/2}(D-A)D^{-1/2} = I-D^{-1/2}AD^{-1/2}$ 의 대각행렬 분해를 $L=U^t \Lambda U$ 로 했을 때, 이 $U$에 대한 graph fourier transformation의 정의에 따라,
        $g \star x := \mathcal{F}^{-1}(\mathcal{F}( g) \circ \mathcal{F}(x))$ 이라 자연스레 정의할 수 있다. 다만, 여기서 $\circ$ 는 두 벡터의 term by term 곱 연산.
        그럼, $\mathcal{F}^{-1}(\mathcal{F}(g) \circ \mathcal{F}(x)) = U ( U^tg \circ U^tx) = U ( diag(U^tg) U^tx)$ 이 된다. 즉, 우리는 $g \star x := U ( diag(U^tg) U^tx)$  를 얻었다.
        
        이때, 앞서 말한 것 처럼 $diag(U^tg)$는 $g$를 직교 eigen-vector들에 대한 projection coefficient를 구한 것이라고 볼 수 있는데,
        $diag(g)$와 $diag(U^tg)$는 좌표계 차이 밖에 나지 않으므로 
        일반성을 잃지 않고 **우리는 원래부터 $g$가 $U^t$ 좌표계에서 표현된 녀석이라고 가정해도 충분**하다. (증명을 하는 것이 아닌, 적절한 정의를 세우는  중이므로 딱히 문제 없어 보인다.)
        
        결론: 이러한, 모티브 차용하여, $g\_{\theta} \star x = U g\_{\theta}U^t x$ 라는 공식을 정의했다고 볼 수 있다.
            
           
            
    - **두번째 STEP**
        
        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP2.png?raw=true" width="800" height="200"/>
        
        💡 그런데, 실제의 계산에 있어서는 대각행렬분해 하는 것이 쉽지 않다.
        좀더 정확히 말하면, eigen-value는 어느정도 구할 수 있지만 $U$를 구체적으로 구하는 것이 계산이 많이드는 어려운 일이라고 한다.
        그래서, 사실 $U g\_{\theta}U^t x$을 계산하기란 큰 graph $G$에 대해서는 매우 어려운 일인데,
        Hammond (2011) 는, eigenvalue의 대각행렬 $Λ$에 대해서 **임의의 벡터** $g\_{θ'}$ 를 $g\_{θ'} ≈ \sum\_{k=1}^Kθ'_k  T_k(\tilde{Λ})$로 잘 근사할 수 있음을 보인바가 있다.
          
        
          
    - **세번째 STEP**
        
        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP3.png?raw=true" width="800" height="200"/>
        
       
        💡 기존의 공식 $U g_{\theta}(\Lambda)U^t x$에서 위의 approximation을 대입해보면, 
        
        $U g\_{\theta'}(\Lambda)U^t x≈U^t(\sum\_{k=1}^Kθ'_k  T_k(\tilde{Λ}))U x =  (\sum\_{k=1}^Kθ'_k U^t T_k(\tilde{Λ})U) x$  인데, 
        
        간단한 계산으로부터 $U \tilde{\Lambda}U^t = \tilde{L}$ 임을 알 수 있고, 이를 이용해 간단히 수학적 귀납법으로 $U T_k(\tilde{\Lambda})U^t = T_k(\tilde{L})$임을 알 수 있다. 이제 이 식을 위에 대입하면, $U g\_{\theta'}(\Lambda)U^t x ≈  (\sum\_{k=1}^Kθ'_k  T_k(\tilde{L})) x$  이 된다.
            
           
            
    - **네번째 STEP**
        
        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP4.png?raw=true" width="800" height="200"/>
        
        
        💡 그리고 조금 더 간편화 하기위한 가정으로
        $\lambda\_{max}=2$ 그리고 $K=1$ 인 경우 (즉, $\sum\_{k=0}^K$ 에서 $K= 1$ 인 경우) 를 생각해본다. 그럼 자연스럽게 $\tilde{L} = L - I_N$ 이 되므로, 
        
        $g\_{θ'} \star x ≈ θ'_0 x + θ'_1 (L - I_N)x = θ'_0 x - θ'_1 D^{-1/2}AD^{-1/2}x$
        
            
    - **다섯번째 STEP**
        
        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP5.png?raw=true" width="800" height="200"/>
        
            
        💡 또한 간편화를 위한 설정으로, $\theta=\theta'_0=-\theta'_1$ 라고 가정하자.( **간편화를 많이 하네..?** 하지만 모티브를 얻어 Neural Network 구조를 정의하고자 하는 관점에서는 정당한 방법론으로 보인다) 그러면 이제 논문에서 새로 정의하려는 convolution 연산은 충분히 간단화 된 것 같다.
        
        >근데 $I_N + D^{-1/2}AD^{-1/2}$의 eigenvalue를 $\lambda$라고 할때, 
        $0 \le  \lambda \le 2$ 이다 (**why? 밑에 증명**).
        즉, $1$보다 큰 $\lambda$에 대응되는 eigen-vector $x$에 대해서 계속 위의 식을 반복적으로 적용한다면, 그 결과는 unstable하게 커지게 된다. ($\lambda$ 배 만큼 계속 커진다)
        

            
        <details>
        <summary> 나 처럼 증명이 너무 궁금한 사람만.. (펼치기/접기) </summary>

        <div markdown="1">

        > **[$I_N + D^{-1/2}AD^{-1/2}$ 의 eigenvalue를 $\lambda$ 라고 할때, $0 \le \lambda \le 2$ 임을 보이는 증명]**
        >
        >❓ (증명) 먼저 $B=D+A$ 라고 하자. 그럼 임의의 벡터 $x \in \mathbb{R}^N$에 대하여,
        > 
        >$x^tBx = x^tDx +x^tAx = \sum_i D\_{ii}x_i^2 + \sum\_{ij} x_i A\_{ij}x_j$
        >
        >$= \frac{1}{2}\left( 2\sum_i D\_{ii}x_i^2 + 2\sum\_{ij} x_i A\_{ij}x_j \right)   = \frac{1}{2}\left( 2\sum_i (\sum_j A\_{ij})x_i^2 + 2\sum\_{ij} x_i A\_{ij}x_j \right)$ 
        > 
        >$= \frac{1}{2}\left( \sum\_{ij}  A\_{ij}x_i^2+ \sum\_{ij}  A\_{ij}x_i^2 + 2\sum\_{ij} x_i A\_{ij}x_j \right)$ 
        > 
        >$= \frac{1}{2} \sum\_{ij} A\_{ij} (x_i + y_i)^2 = \sum\_{(u,v) \in E(G)}  (x_u + y_v)^2$
        > 
        >그럼 이제, $\tilde{B}:=D^{-1/2}BD^{-1/2} (=I_N + D^{-1/2}AD^{-1/2})$로 두자.
        >
        >어떤 $v \neq 0$에 대하여 $\tilde{B}v= \lambda v$라 하자. 
        > 
        >그럼, $v^t\tilde{B}v = v^t(\lambda v) = \lambda v^tv$  이므로 eigenvalue $\lambda$는 다음과 같다.
        >
        >$$\begin{aligned}
        \lambda  &= \frac{v^t\tilde{B}v}{v^tv } \\
        &= \frac{v^tD^{-1/2}BD^{-1/2}v}{v^tv }\\
        &= \frac{x^tBx}{x^tDx } & \text{if we take $x:= D^{-1/2}v$}\\
        &= \frac{\sum\_{(u,v) \in E(G)}  (x_u + x_v)^2}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)} & \text{so $\lambda \ge 0$}\\
        \end{aligned}$$
        >
        >그런데, $(x_u+x_v)^2 \leq 2(x_u^2+x_v^2)$가 항상 성립하고 등호일 조건은 모든 $(u,v) \in E(G)$에 대해 $x_u=x_v$이다.
        그러면 그러한 최대값 등호를 모두 가지는 상황의 $x$에 대해서는 
        >$$ \begin{aligned}
        \lambda_{max} &= \frac{\sum\_{(u,v) \in E(G)}  (x_u + x_v)^2}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)}\\
        &= \frac{\sum\_{(u,v) \in E(G)}  2(x_u^2 + x_v^2)}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)} & \text{because  $(x_u+x_v)^2 = 2(x_u^2+x_v^2)$}\\
        &= \frac{\sum\_{u,v  \text{ with } (u,v) \in E}  (x_u^2 + x_v^2)}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)} = \frac{\sum\_{u,v  \text{ with } (u,v) \in E }  (2x_v^2 )}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)} \\
        &= \frac{2\sum\_{v }  x_v^2 \cdot deg(v)}{\sum\_{v \in V(G)}  x_v^2 \cdot deg(v)} = 2 \\
        \end{aligned}$$
        >
        >따라서 $\lambda\_{max} \le 2$ 이므로 증명 완료 ...
        </div>
        </details>        
        
        <br>
        
        따라서, stable하도록 모델링 하기 위하여 우리는  $I_N + D^{-1/2}AD^{-1/2} \rightarrow \tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ 로 생각하기로 한다.
        
        **(개인적으로 생각하는 이 치환이 나름 정당한 이유는? )** 
        $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} = I_N + \tilde{D}^{-1/2}A\tilde{D}^{-1/2}$인데, 애초에 우리의 normalized Laplacian의 정의를 $\tilde{L} := \tilde{D}^{-1/2}(D-A)\tilde{D}^{-1/2}$로 잡고 처음부터 이론을 전개했다고 하면 되니까!       
             
               
                
    - **마지막 단계** 
     
        따라서, 우리는 single parameter $\theta$에 대한 두 벡터 $g\_{\theta},x$ 에 대한 convolution 연산의 정의를 

        $g\_{\theta} \star x := \theta (\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}) x$ 로 정의하였고 이를 다변수 형식으로 정의하면 다음과 같다.

        <img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/STEP6.png?raw=true" width="800" height="200"/>

<br>
<br>

그래서 우리는 두 번째 목적이었던

>2. 그런데 왜 이게 **Convolutional Network**라고 불리우는 것인지 이해하고, 논문의 공식 도출 과정을 전반적으로 이해한다.

를 드디어 이루었다..!

## **참고 문헌 및 자료**
1. Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016).
   
2. David K. Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.

3. [https://ralasun.github.io/deep learning/2021/02/15/gcn/](https://ralasun.github.io/deep%20learning/2021/02/15/gcn/)

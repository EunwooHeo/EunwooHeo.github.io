---
layout: single
title:  "[PAPER 리뷰] The Graph Neural Network Model"
---

**저자:   Franco Scarselli,  Marco Gori etc**

오늘은 현재 활발히 연구되고 있는 The Graph Neural Network(이하 GNN)의 시초가 된 논문이라고 여겨지는 논문에 대해 리뷰하고자 한다.

<details>
<summary>Remark (펼치기/접기)</summary>
<div markdown="1">

이 논문이 GNN의 시초가 되는 논문이라고 누군가 얘기하였다. 여기서 도입이라 얘기하는 것은 message passing update과정을 쓴 시초 논문이라는 것일까?
Graph위에서 Neural Network를 시도하는 연구는 이 논문의 이전에도 활발히 이루어지고 있었기 때문에... 
그냥 GNN의 시초라는 것은 나에겐 애매한 말로 들린다. 
그래서 나는, 단지 그래프 위에서 데이터(data)를 학습시키고 비용 함수(cost function)에 대해 gradient descent하는 방식의 선행연구는 이미 활발히 진행 되었지만, 그래프의 성질을 반영해서 update하는 과정은 이 논문에서 도입하였기 때문에 시초구나라고 여기고 스스로 넘어간다. (틀린 정보면 알려주시길..)

</div>
</details>

<br>

>논문의 핵심만 간단히 얘기하자.
이번 논문의 핵심은, 평소 비용함수(cost function)를 이용한 Neural Network의 학습 과정 처럼 똑같이 하되,
그 과정 사이 사이에 그래프 본연의 정보를 얻기 위해 그래프 정보를 서로 주고 받으며 업데이트 하는 과정을 넣자.

그리드(grid) 형식으로 되어있는 이미지 데이터에서 NN(neural network)는 딥러닝 모델의 핵심이다. 이때의 이미지 데이터는 격자 그리드의 각 점에 state $x_n$ (또는 feature $x_n$)을 붙여 놓은 것으로 여길 수 있다. 그러면 똑같은 방법론을 그리드 형식의 데이터 말고 그래프 데이터로는 할 수 없을까? 현실에서는 그리드 데이터로는 표현하기 힘든 정보들이 많다. 분명 현실에서는 "**그래프**"가 데이터의 표현법으로 가장 적합한 경우가 있을 것이다. 그렇다면 그래프 위에서 NN(neural network)를 해보고 싶은 것은 자연스러운 흐름이라 할 수 있다. 그런데 그냥 그리드 데이터에서 하듯이 하는 것은 성능이 꽤 좋지 않았다고 한다. 따라서 저자는 다음과 같은 '**그래프 본연의 성질**'을 어느정도 담아내는 방식을 소개 한다. 

<br>

 그래프 데이터에서는 vertex가 바로 이 격자점 역할을 한다. 그리고 각 vertex $n \in \mathbb{N}$ (자연수 집합)에 대하여 아래 그림과 같이 state와 label이 붙어있는 상황이라고 생각하자. 
 
 > 직관 더하기) 여기서 state는 그 점(vertex), 변(edge) 등 의 특성을 말하고자 하는 벡터(vector)라 여기면 되고, label은 점의 분류를 위한 어떤 값이라 여기면 된다.

<br>
<br>

## **논문 내용  정리**
    
<img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/pic_massage.png?raw=true" width="50%" height="50%">

상황: 각 vertex $n$에는 state $x_n$과 label $l_n$이 붙어있다. (edge도 마찬가지)

이때 state $x_n$과 output $o_n$은 다음과 같이 정의 된다.

> $x_n=f_w(l_n ,l_{co[n]},l_{ne[n]},x_{ne[n]})$, $\;$ $o_n=g_w(x_n,l_n)$
> 
- $f_w$는 local transition function , $g_w$ : local output function 이라 한다.
- $l_n$  은 vertex $n$의 label
- $l_{co[n]}$ 은 $co[n]$ 라 불리는 edge의 label
- $l_{ne[n]}$은 neighborhool vertex들의 label
- $x_{ne[n]}$은 neighborhood vertex들의 state

이 과정에 따라 순서대로 계속해서 vertex들의 $x_n$과 $o_n$을 업데이트 시킨다.

*여기서 output은 또 뭐지? 라고 여겨 진다면, 밑에서 소개할 "**계산을 위한 어떤 값**"이라고 잠시 생각하자. 

이렇게 업데이트 하는 정보를 가지고, 
$x,o,l,l_N$을 각각 모든 state , 모든 output, 모든 label, 모든 vertex 라벨을 각각 모은 것이라 정의하고, 논문에서는 위의 과정을

> $x=F_w(x,l)$ ,    $o=G_w(x,l_N)$


이라고 정의했다. 
이때, $F_w$는 the global transition function , $G_w$는 the global output function 라 한다.

이 논문에서는 Banach’s theorem에 따라서, 만약 $F_w$가 a contraction map이면,
즉, 임의의 $x,y,l$에 대하여 $\lVert F_w(x,l) - F_w(y,l) \rVert \le \mu \lVert x - y \rVert$ for some $\mu \in [0,1)$ 이라면,
위의 과정을 반복적으로 시행했을 때, 유일한 $o, x$에 도달할 수 있다고 하였다.

좀 더 자세히 말하면, $x_{n+1}=F_w(x_n,l_n), o_{n+1}=G_w(x_n,(l_N)_n)$ 식으로 재귀적으로 반복을 하다보면 어느새 고정된 유일한 $o, x$에 수렴한다는 것이다.

이때, 그 유일한 값으로 보내는 함수 $\varphi_w:\mathcal{D} \rightarrow \mathbb{R}^m$  을 $G \mapsto o_G$  정의하자. 
즉, $\varphi_w$는 $F_w, G_w$  로부터 결정되는 함수이다.

### 그럼 파라미터 $w$에 대해 학습을 어떻게 시킨다는 것일까?

저자는 많은 그래프 데이터 $G_i$ 들과 각 vertex들 $n_{ij}$의 target $t_{ij}$들에 대한 
Learning data set $\mathcal{L}$을 다음과 같이 표현했다.

> $\mathcal{L} = \{ (G_i,n_{ij},t_{ij}  \mid G_i \in \mathcal{G}, n_{ij} \in V(G_i), t_{ij} \in \mathbb{R}^m \}$
> 

그리고 a quadratic cost function $c_w$를 다음과 같이 정의했다.

> $c_w= \sum_i \sum_j (t_{ij} - \varphi_w(G_i,n_{ij}))^2$
> 

이제 이 cost function에 대한 gradient descent를 하여 $w$를 업데이트 한다.
그리고 $w$가 업데이트 되는 매순간 $\varphi_w(G_i,n_{ij})$가 계산되어야 하므로 
message passing과정  $x=F_w(x,l)$ , $o=G_w(x,l_N)$ 가 이루어진다.

(각 message passing 과정에서 $x,o$를 구하기 위해, 수렴할 때 까지 무한히 하진 않고, 적당히 충분히 반복했다 싶으면 넘어가겠지..)

참고로, 논문에서는 다음과 같은 정리를 증명했다.

> If $F_w, G_w \in C^1$ then $\varphi_w \in C^1$
> 

또한 Backpropagation을 위해 다음 정리를 증명했다.

> Let $F_w, G_w \in C^1$ and $z(t)=z(t+1) \cdot\frac{\partial F_w}{\partial x}(x,l) +  \frac{\partial c_w}{\partial o}\cdot\frac{\partial G_w}{\partial x}(x,l_N)$.
Then $\exists z_0 := \lim_{t \rightarrow - \infty} z(t)$ 
and $\frac{\partial c_w}{\partial w}= z_0 \cdot\frac{\partial F_w}{\partial w}(x,l) +  \frac{\partial c_w}{\partial o}\cdot\frac{\partial G_w}{\partial x}(x,l_N)$
> 

공식을 보면, 식의 중간에 $x, o, l$ 등이 쓰이는 것을 볼 수 있는데,
이러한 값들은 매번 업데이트 순간에 쓰이게 된다.
그것이 의미하는 바는 다음과 같다

> (기존의 방식과는 달리) 매 업데이트 순간 그래프 데이터 성질을 어느 정도 잘 반영한 값들(예를 들면, $x$, $o$, $l$ 등)이 식에 잘 녹아 있으므로, 그래프의 특징을 어느정도 반영하여 계산을 한다.


이상, GNN 논문 리뷰를 위한 첫 블로그 포스팅을 마칩니다. 

## **참고 문헌 및 자료**
1. Scarselli, Franco, et al. "The graph neural network model." IEEE transactions on neural networks 20.1 (2008): 61-80.
    

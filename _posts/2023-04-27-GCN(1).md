---
layout: single
title:  "[PAPER 리뷰] (Part 1) SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS"
---

**저자:   Thomas N. Kipf ,  Max Welling** 


이번에 분석해 볼 논문은 Graph Convolutional Networks(이하 GCN) 논문이다.
복잡했던 Message passing과정을 깔끔하게 잘 설명하고, Graph위의 합리적인 CNN모델을 제시 하면서, 
이슈를 불러일으킨 논문이라고 한다.

수학적으로 내가 만족할만큼 이 논문을 분석한 글을 아직은 찾지 못하였다.

부족하지만, 나만의 뇌피셜로 그것을 한 번 시도 해보고자 한다.

## **우선 결론만 간단 정리**
- 기호 정리
    - $G$ : the undirected graph
    - $A$ : the adjacency matrix
    - $\tilde{A} = A + I$ , where $I$ is the identity matrix.
    - $D$ is the degree diagonal matrix , that is $D\_{ii} = \sum_j A\_{ij}$ for all $i$
    - $\tilde{D} = D+I$   or $\tilde{D}\_{ii} = \sum_j \tilde{A}\_{ij}$ for all $i$.
    - $σ(·)$ : an activation function.
    - for vertex $v \in V(G)$, let $x_v$ be a feature vector in $\mathbb{R}^C$.
    - $X = (x\_{v_1}, x\_{v_2}, \ldots , x\_{v_m})^t$ , that is each $x_v$ is a row vector of $X$.
- $f(X,A)$ : The specific graph-based neural network model 이라고하자.

- 결론적으로, GCN 모델의 layer-wise propagation rule은 다음과 같다.
    
    > $H^{(l+1)} = σ(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$ 
    with $H^{(0)} = X$.

GCN 모델은 그 명성에 비하면 정말 간단하게 생겼다.
각 단계의 $H^{(l)}$의 왼쪽에는 nomalized adjacency matrix $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ 를 곱해주고, 오른쪽에는 학습시킬 weight matrix $W^{(l)}$를 곱해준다. 그리고 activiation function $σ(·)$ 을 씌운다. 이게 이 모델의 끝이다. 

**이게.. 끝?**

이번 논문을 분석하면서 독자 여러분께 제공드리고자 하는 목표는 다음과 같다.

>1. 위에 제시된 모델에서 각 단계의 $H^{(l)}$ 의 왼쪽에는 $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$를 오른쪽에 $W^{(l)}$를 곱하는 것의 수학적인 직관 제공을 한다. 따라서 앞으로 모든 GNN 관련 논문을 읽기에 용이하도록 한다.
>
>2. 그런데 왜 이게 **Convolutional Network**라고 불리우는 것인지 이해하고, 논문의 공식 도출 과정을 전반적으로 이해한다.

<br>
<br>

그럼 시작해보자.

<!-- <details>
<summary>Remark (펼치기/접기)</summary>
<div markdown="1">

as

</div>
</details> -->

## 0. Backgroud
---
우선 1. $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$는 무슨 의미일까? (보통 이러한 과정을  Normalization이라고 한다.)


<details>
<summary> 예시를 통하여 normalization의 의미 와닿게 해보기(펼치기/접기) </summary>

<div markdown="1">

<img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/graph.png?raw=true" width="300" height="300">

- 위의 그래프에 해당하는 인접행렬 (Adjacency matrix) 는
$A = \begin{bmatrix}
0 & 1 & 0 & 1 & 0 \\\
1 & 0 & 1 & 0 & 0 \\\
0 & 1 & 0 & 1 & 1 \\\
1 & 0 & 1 & 0 & 0 \\\
0 & 0 & 1 & 0 & 0
\end{bmatrix}$ 그리고 각 점의 Degree matrix는 
$D = \begin{bmatrix}
2 & 0 & 0 & 0 & 0 \\\
0 & 2 & 0 & 0 & 0 \\\
0 & 0 & 3 & 0 & 0 \\\
0 & 0 & 0 & 2 & 0 \\\
0 & 0 & 0 & 0 & 1
\end{bmatrix}$ 이고 
$\tilde{D}=D+I = \begin{bmatrix}
3 & 0 & 0 & 0 & 0 \\\
0 & 3 & 0 & 0 & 0 \\\
0 & 0 & 4 & 0 & 0 \\\
0 & 0 & 0 & 3 & 0 \\\
0 & 0 & 0 & 0 & 2
\end{bmatrix}$.

- 이제
$\tilde{D}^{-1/2} = \begin{bmatrix}
  \frac{1}{\sqrt{3}} & 0 & 0 & 0 & 0 \\\
0 & \frac{1}{\sqrt{3}} & 0 & 0 & 0 \\\
0 & 0 & \frac{1}{\sqrt{4}} & 0 & 0 \\\
0 & 0 & 0 & \frac{1}{\sqrt{3}} & 0 \\\
0 & 0 & 0 & 0 & \frac{1}{\sqrt{2}}
\end{bmatrix}$ 그리고,
$\tilde{D}^{-1/2}  (A+I) \tilde{D}^{-1/2}  = \begin{bmatrix}
\frac{1}{\sqrt{3}\sqrt{3}} & \frac{1}{\sqrt{3}\sqrt{3}} & 0 & \frac{1}{\sqrt{3}\sqrt{3}} & 0 \\\
\frac{1}{\sqrt{3}\sqrt{3}} & \frac{1}{\sqrt{3}\sqrt{3}} & \frac{1}{\sqrt{3}\sqrt{4}} & 0 & 0 \\\
0 & \frac{1}{\sqrt{4}\sqrt{3}} & \frac{1}{\sqrt{4}\sqrt{4}} & \frac{1}{\sqrt{4}\sqrt{3}} & \frac{1}{\sqrt{4}\sqrt{2}} \\\
\frac{1}{\sqrt{3}\sqrt{3}} & 0 & \frac{1}{\sqrt{3}\sqrt{4}} & \frac{1}{\sqrt{3}\sqrt{3}} & 0 \\\
0 & 0 & \frac{1}{\sqrt{2}\sqrt{4}} & 0 & \frac{1}{\sqrt{2}\sqrt{2}}
\end{bmatrix}$

> 즉, $A+I$의 $(i,j)$성분에 $\frac{1}{\sqrt{(d_i+1) \cdot (d_j+1)}}$ 를 곱해주는 것이라고 생각하면 된다. 일반화 해보면, 앞 뒤로 행렬을 degree행렬을 곱하는 결과는 다음과 같이 정리될 수 있다.
>
>For matrix $B=(b_{ij})$ and degree matrix $D$, ${D}^{-1/2} B {D}^{-1/2} = (\frac{1}{\sqrt{d_i}\sqrt{d_j}}b_{ij})$ and $\tilde{D}^{-1/2} B \tilde{D}^{-1/2} = (\frac{1}{\sqrt{d_i+1}\sqrt{d_j+1}}b_{ij})$ 
> 

</div>
</details>

그렇다면, 이제는 독자 여러분께서는 이것의 의미를 한번 생각해보시면 좋을 것 같다. 각 점의 degree가 높을 수록 오히려 낮은 값을 취하게 하는 것이, 무언가를 어느정도 평탄화 시키는 과정, 즉 Normalization이라고 받아들여지는가?


---

다음으로, 이제 이러한 Normalization의 결과물인 $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$를 왼쪽에 곱하는 것은 어떤 것을 의미할까?



<details>
<summary> 예시를 통하여 인접 행렬을 왼쪽에 곱하는 의미 와닿게 해보기 (펼치기/접기)</summary>
<div markdown="1">

<img src="https://github.com/math-hew/math-hew.github.io/blob/master/_posts/image/graph.png?raw=true" width="300" height="300"/>

- 동일하게, 위 그래프의 인접행렬 (Adjacency matrix)은
$A = \begin{bmatrix}
0 & 1 & 0 & 1 & 0 \\\
1 & 0 & 1 & 0 & 0 \\\
0 & 1 & 0 & 1 & 1 \\\
1 & 0 & 1 & 0 & 0 \\\
0 & 0 & 1 & 0 & 0
\end{bmatrix}$ 이고, $\tilde{A}=A + I = \begin{bmatrix}
1 & 1 & 0 & 1 & 0 \\\
1 & 1 & 1 & 0 & 0 \\\
0 & 1 & 1 & 1 & 1 \\\
1 & 0 & 1 & 1 & 0 \\\
0 & 0 & 1 & 0 & 1
\end{bmatrix}$.

$X=\begin{bmatrix}
x_{11} & x_{12} & x_{13}  \\\
x_{21} & x_{22} & x_{23} \\\
x_{31} & x_{32} & x_{33}  \\\
x_{41} & x_{42} & x_{43} \\\
x_{51} & x_{52} & x_{53}  \\\
\end{bmatrix}
=\begin{bmatrix}
\vec{v}_1  \\\
\vec{v}_2  \\\
\vec{v}_3   \\\
\vec{v}_4  \\\
\vec{v}_5   \\\
\end{bmatrix}$ 로 각 vertex에 3차원 $\vec{v}_i$벡터가 feature vector로 부여되어있다고 하자. 

그럼 $\tilde{A}\cdot X$는 다음과 같다.

$\tilde{A}\cdot X = \begin{bmatrix}
x_{11} + x_{21} + x_{41} & x_{12} + x_{22} + x_{32} & x_{13} + x _{23} + x_{33} \\\
x_{11} + x_{21} + x_{31} & x_{12} + x_{22} + x_{23} & x_{13} + x _{23} + x_{43} \\\
x_{21} + x_{31} + x_{41} + x_{51} & x_{22} + x_{32} + x_{42} + x_{52}&x_{23} + x_{33} + x_{43} + x_{53} \\\
x_{11} + x_{31} + x_{41} & x_{13} + x_{32} + x_{42} & x_{11} + x _{33} + x_{43} \\\
x_{31} + x_{51} & x_{32} + x_{52} & x_{33} + x_{53}
\end{bmatrix}
= \begin{bmatrix}
\vec{v}_1+\vec{v}_2+\vec{v}_4\\\
\vec{v}_1+\vec{v}_2+\vec{v}_3\\\
\vec{v}_2+\vec{v}_3+\vec{v}_4+\vec{v}_5\\\
\vec{v}_1+\vec{v}_3+\vec{v}_4\\\
\vec{v}_3+\vec{v}_5\\\
\end{bmatrix}$

> 즉, 앞으로 인접행렬과 같은 binary matrix를 $X$의 좌측에 곱하는 행동은 $1$값에 해당하는 index의 row 끼리 모아서 더해주는 것이라 생각하면 된다. 따라서 인접행렬 $A$에 대해 $A \cdot X$는 자신을 제외하고 자신과 연결된 점들의 feature vector를 합해주는 것이다. 또한 $\tilde{A} \cdot X$는 자신을 포함하여 자신과 연결된 점들의 feature vector를 합해주는 것이다.
> 

</div>
</details>

그렇다면, $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} \cdot X$는 무엇을 의미하는지 이제 아시겠는가? 독자 여러분께 맡긴다.

이로써, 이번 글의 첫번째 목적
>1. 위에 제시된 모델에서 각 단계의 $H^{(l)}$ 의 왼쪽에는 $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$를 오른쪽에 $W^{(l)}$를 곱하는 것의 수학적인 직관 제공을 한다. 따라서 앞으로 모든 GNN 관련 논문을 읽기에 용이하도록 한다.

은 어느정도 달성된 것 같다. 그런데 쓰다보니 너무 글을 길게 쓴 것 같아서 다음 편에 계속..



## **참고 문헌 및 자료**
1. Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016).
